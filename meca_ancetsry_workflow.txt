# Step 1: Prepare Reference.VCF File
# Download 1000Genome Reference Chromosome 1 to 22 (vcf.gz and vcf.gz.tbi)
wget -r -np -nH --cut-dirs=6 -R "index.html*" ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/phase3_liftover_nygc_dir/

# Concate all vcf.gz files
bcftools concat *.vcf.gz -o concat_subset_ref_all.vcf

# Extract interest of reference samples from 1000 Genome reference samples
# Step 1: download sample file from 1000Genome data portal and write sample_ID in sample_id_ref.txt file (one sample one line, no header needed)
# Step 2: Write a Script "extract_ref_sample.sh" and give permission to it. Run it in nohup to generate the interest_ref.vcf file 
# script_1
#!/bin/bash

# Input files
VCF_FILE="concat_subset_ref_all.vcf"   # Replace with the path to your 1000 Genomes VCF file
SAMPLE_FILE="sample_id_ref_portal.txt"      # File containing sample IDs of interest (one per line) N=1617
OUTPUT_FILE="extracted_samples.vcf.gz"
MISSING_SAMPLES_LOG="missing_samples.log"

# Check if input files exist
if [[ ! -f "$VCF_FILE" ]]; then
    echo "Error: VCF file '$VCF_FILE' not found!"
    exit 1
fi

if [[ ! -f "$SAMPLE_FILE" ]]; then
    echo "Error: Sample ID file '$SAMPLE_FILE' not found!"
    exit 1
fi

# Clear any previous logs
> "$MISSING_SAMPLES_LOG"

# Extract sample IDs from the sample_id_ref_portal.txt and process each sample
SAMPLES=""
while read -r SAMPLE; do
    if bcftools view -h "$VCF_FILE" | grep -q "$SAMPLE"; then
        echo "Sample $SAMPLE found in VCF. Adding to list..."
        SAMPLES="${SAMPLES},${SAMPLE}"
    else
        echo "Sample $SAMPLE not found in VCF. Logging missing sample."
        echo "$SAMPLE" >> "$MISSING_SAMPLES_LOG"
    fi
done < <(tail -n +2 "$SAMPLE_FILE")  # Ignore header in the sample file

# Remove leading comma from sample list
SAMPLES=${SAMPLES#,}

# Extract all valid samples in one go
if [[ -n "$SAMPLES" ]]; then
    bcftools view -s "$SAMPLES" "$VCF_FILE" -Oz -o "$OUTPUT_FILE"
    bcftools index "$OUTPUT_FILE"
    echo "Samples extracted and saved in $OUTPUT_FILE."
else
    echo "No valid samples found."
fi

echo "Missing samples (if any) are logged in $MISSING_SAMPLES_LOG."

# reconfirm chromosome no. and sample ID before moving forward.
bcftools query -f '%CHROM\n' extracted_samples.vcf.gz|uniq									(# Check all chromosomes present or not)
bcftools query -l extracted_samples.vcf.gz |uniq |wc -l										(# N=1249*)
bcftools view -h extracted_samples.vcf.gz | grep '^#CHROM' > ref_header.txt					(# Check sample identity and validate above number of samples*)

# Modify chromosome IDs in reference file
## prepare chromosome.txt file
bcftools query -f '%CHROM\n' extracted_samples.vcf.gz|uniq > chromosomes.txt				(# chromosome.txt should be tab seperated)
nano chromosomes.txt
chr10   10
chr11   11
chr12   12
chr13   13
chr14   14
chr15   15
chr16   16
chr17   17
chr18   18
chr19   19
chr1    1
chr20   20
chr21   21
chr22   22
chr2    2
chr3    3
chr4    4
chr5    5
chr6    6
chr7    7
chr8    8
chr9    9
## rename the chromosome
bcftools annotate --rename-chrs chromosomes.txt extracted_samples.vcf.gz -Oz -o ref_edit.vcf.gz						(#81570102)
gunzip ref_edit.vcf.gz

# step 2: prepare meca.vcf_file
# merge visit_1 and visit_2 gvcf file
bcftools merge --force-samples -o meca_visit_1_all_gvcf.vcf -Oz *.vcf.gz
bcftools merge --force-samples -o meca_visit_2_all_gvcf.vcf -Oz *.vcf.gz

# Index files
tabix -p vcf meca_visit_1_all_gvcf.vcf.gz
tabix -p vcf meca_visit_2_all_gvcf.vcf.gz

# Convert gvcf to vcf
GATK --java-options "-Xmx100G" GenotypeGVCFs -R /REF/REFERENCES/GRCH38_MainChrs/Homo_sapiens.GRCh38.dna.mainchrs.fa -V meca_visit_1_all_gvcf.vcf.gz -O meca_visit_1_combine_all.vcf.gz &
GATK --java-options "-Xmx100G" GenotypeGVCFs -R /REF/REFERENCES/GRCH38_MainChrs/Homo_sapiens.GRCh38.dna.mainchrs.fa -V meca_visit_2_all_gvcf.vcf.gz -O meca_visit_2_combine_all.vcf.gz &

# Merge visit 1 and visit 2 samples
bcftools merge --force-samples -o meca_visit_1_2.vcf.gz -Oz meca_visit_1_combine_all.vcf.gz meca_visit_2_combine_all.vcf.gz

# Rename the samples
bcftools reheader -s rename_sample.txt -o meca_visit_1_2_rename.vcf.gz meca_visit_1_2.vcf.gz

# Extract Genotype infromation (GT)from Format to  make it simple
bcftools annotate -O v -x ^FORMAT/GT meca_visit_1_2_rename.vcf.gz > meca_GT.vcf

# Count the number of variants in the original VCF file.
bcftools view -H meca_GT.vcf |wc -l									(#23054457)

# Filter the VCF file for variants 
bcftools filter -i 'QUAL > 30 & DP > 10' meca_GT.vcf -o meca_GT_Qual.vcf
bcftools view -H meca_GT_Qual.vcf |wc -l							(#5040259)

# Extract only SNP variants from the filtered VCF
bcftools view -v snps meca_GT_Qual.vcf -o meca_GT_snps.vcf
bcftools view -H meca_GT_snps.vcf |wc -l							(#4006853)

# Filter SNPs with allele frequency
bcftools filter -i 'AF > 0.05' meca_GT_snps.vcf -o high_af_snps.vcf
bcftools view -H high_af_snps.vcf |wc -l							(#3149289)

# Compress and index the high allele frequency SNPs VCF file
bgzip high_af_snps.vcf
tabix -p vcf high_af_snps.vcf.gz

# Annotate the high allele frequency SNPs
bcftools annotate -a ../dbsnp138_edit_sorted.vcf.gz -c CHROM,POS,ID,REF,ALT -o annotated_d.vcf high_af_snps.vcf.gz
bgzip annotated_d.vcf
tabix -p vcf annotated_d.vcf.gz
bcftools annotate -a ../hapmap_3.3.hg38_edit.vcf.gz -c CHROM,POS,ID,REF,ALT -o annotated_h.vcf annotated_d.vcf.gz
bgzip annotated_h.vcf
tabix -p vcf annotated_h.vcf.gz
bcftools annotate -a ../Mills_and_1000G_gold_standard.indels.hg38_edit.vcf.gz -c CHROM,POS,ID,REF,ALT -o annotated_m.vcf annotated_h.vcf.gz
bgzip annotated_m.vcf
tabix -p vcf annotated_m.vcf.gz

# Step_3 quality control of query an dreference file
## Query
bcftools view -O z -o meca_AC3.vcf.gz -i 'AC>3' annotated_m.vcf.gz
tabix -f meca_AC3.vcf.gz
bcftools view -O z -o meca_2alleles.vcf.gz --max-alleles 2 meca_AC3.vcf.gz
tabix -f meca_2alleles.vcf.gz
bcftools norm -d both -o meca_both.vcf.gz -Oz meca_2alleles.vcf.gz

## References
bcftools view -O z -o ref_AC3.vcf.gz -i 'AC>3' ref_edit.vcf.gz
tabix -f ref_AC3.vcf.gz
bcftools view -O z -o ref_2alleles.vcf.gz --max-alleles 2 ref_AC3..vcf.gz
tabix -f ref_2alleles.vcf.gz
bcftools norm -d both -o ref_both.vcf.gz -Oz ref_2alleles.vcf.gz

# Step 4: Remove bad variants from query files
vcf-validator meca_both.vcf.gz > validator_output.txt 2>&1
grep "Bad ALT value" validator_output.txt > bad_alt_errors.txt
awk -F'[: ]' '{print $4"\t"$5}' bad_alt_errors.txt > remove_variants.txt
bcftools view -T ^remove_variants.txt meca_both.vcf.gz -Oz -o meca_variants.vcf.gz

# Step 5: Phasing and imputation
bcftools norm -d all -o ref_all.vcf.gz -Oz ref_both.vcf.gz
java -Xmx200g -jar /usr/bin/beagle.28Jun21.220.jar gt=meca_variants.vcf.gz ref=ref_all.vcf.gz out=meca_imputated impute=true gp=true nthreads=8
bcftools view -i 'INFO/DR2>0.3' meca_imputated.vcf.gz -Oz -o meca_imputated_filtered.vcf.gz
tabix -p vcf meca_imputated_filtered.vcf.gz
bcftools annotate -O v -x ^FORMAT/GT meca_imputated_filtered.vcf.gz > meca_imputated_GT.vcf
gunzip ref_all.vcf.gz

# Step 4: extract chromosome, position, reference, and alternate allele information for reference and Query File
grep -v '^#' meca_imputated_GT.vcf | cut -f1-5 | tr '\t' ':' > query_info.txt			(#3149289)
grep -v '^#' ref_all.vcf | cut -f1-5 | tr '\t' ':' > ref_info.txt				(#81570102)
grep -F -x -f ref_info.txt query_info.txt > common_entries.txt
cat common_entries.txt |wc -l													(#1466344)
sed -i 's/:/\t/g' common_entries.txt

# run a custom Python script to get common varinats from reference and meca vcf file
python script.py

## Script_2
def read_common_variant_file(file_path):
    common_variants = set()
    with open(file_path, 'r') as f:
        for line in f:
            columns = line.strip().split('\t')
            if len(columns) >= 5:
                variant = tuple(columns[:5])  # Assuming columns 1 to 5 contain the information
                common_variants.add(variant)
    return common_variants

def filter_vcf(input_vcf_path, output_vcf_path, common_variants):
    with open(input_vcf_path, 'r') as vcf_in, open(output_vcf_path, 'w') as vcf_out:
        for line in vcf_in:
            if line.startswith('#'):
                vcf_out.write(line)
                continue

            columns = line.strip().split('\t')
            variant = tuple(columns[0:5])  # Assuming columns 1 to 5 contain the information

            if variant in common_variants:
                vcf_out.write(line)

if __name__ == '__main__':
    common_variant_file = 'common_entries.txt'
    input_vcf_file = 'ref_edit.vcf'
    output_vcf_file = 'output_r.vcf'

    common_variants = read_common_variant_file(common_variant_file)
    filter_vcf(input_vcf_file, output_vcf_file, common_variants)

    print("Filtered VCF file created successfully.")
	
# check the number of variants before proceed
bcftools view -H output_r.vcf | wc -l						(#1466344)
bcftools view -H output_q.vcf | wc -l						(#1466344)

# Step 4: Processing of reference and query files
#convert VCF to PLINK format
PLINK --vcf output_q.vcf --make-bed --out meca
PLINK --vcf output_r.vcf --make-bed --out reference

# filter plink files based on minor allele frequency, genotype and hardy-weinberg parameters
PLINK --bfile meca --maf 0.05 --geno 0.5 --hwe 1e-8 midp --make-bed --out meca_filtered				(#1457)(#1977)
PLINK --bfile reference --maf 0.05 --geno 0.5 --hwe 1e-8 midp --make-bed --out ref_filtered				(#713902)(#868473)

# perform LD pruning to remove correlated SNPs
PLINK --bfile meca_filtered --indep-pairwise 100 50 0.2 --out meca_filtered_ld_pruned
PLINK --bfile meca_filtered --extract meca_filtered_ld_pruned.prune.in --export vcf --out meca_filtered_ld
PLINK --bfile ref_filtered --indep-pairwise 100 50 0.2 --out ref_filtered_ld_pruned
PLINK --bfile ref_filtered --extract ref_filtered_ld_pruned.prune.in --export vcf --out ref_filtered_ld

# check no. of variants left
bcftools view -H meca_filtered_ld.vcf |wc -l				(#hwe:4-1236)(#hwe:10-1626)(#skip hwe-307555)
bcftools view -H ref_filtered_ld.vcf |wc -l					(#hwe:4-183112)(#hwe:10-210903)(#skip hwe-495008)

# again find the common variants 
grep -v '^#' ref_filtered_ld.vcf | cut -f1-5 | tr '\t' ':' > ref_info_1.txt
grep -v '^#' meca_filtered_ld.vcf | cut -f1-5 | tr '\t' ':' > query_info_1.txt
grep -F -x -f ref_info_1.txt query_info_1.txt > common_entries_1.txt							(#179)(#301)							
sed -i 's/:/\t/g' common_entries_1.txt
python script.py

# check no. common variants after final filtering
bcftools view -H output_r_1.vcf | wc -l						(#179)(#301)(#72916)
bcftools view -H output_q_1.vcf | wc -l						(#179)(#301)(#72916)

## compress and index the output VCF files
bgzip output_q_1.vcf
bgzip output_r_1.vcf
tabix -p vcf output_q_1.vcf.gz
tabix -p vcf output_r_1.vcf.gz

## merge the meca and reference VCF files
bcftools merge output_q_1.vcf.gz output_r_1.vcf.gz -o merged.vcf
less -S merged.vcf

## make plink file
PLINK --vcf merged.vcf --make-bed --out merged

# update ids in fam file for better understanding on meca and refernce samples
## make update_ids.txt file having information for sample_id, population and sample_label
## makke copy of original file
cp merged.fam merged_original.fam
cp ../common/update_fam.py .
## run python script to update fam
python update_fam.py

#Script_3
# Read the update FID.txt file and create dictionaries of old FID to new FID mapping for both columns
fid_mapping_col1 = {}
fid_mapping_col2 = {}
with open('update_ids.txt', 'r') as update_fid_file:
    for line in update_fid_file:
        old_fid, new_fid_col1, new_fid_col2 = line.strip().split()
        fid_mapping_col1[old_fid] = new_fid_col1
        fid_mapping_col2[old_fid] = new_fid_col2

# Read the original file and create a new content with updated FIDs
new_content = []
with open('merged.fam', 'r') as original_file:
    for line in original_file:
        columns = line.strip().split()
        old_fid = columns[0]
        new_fid_col1 = fid_mapping_col1.get(old_fid, columns[0])
        new_fid_col2 = fid_mapping_col2.get(old_fid, columns[1])
        columns[0] = new_fid_col1
        columns[1] = new_fid_col2
        new_line = ' '.join(columns) + '\n'
        new_content.append(new_line)

# Write the new content back to the original file
with open('merged.fam', 'w') as original_file:
    original_file.writelines(new_content)

print("FIDs in both columns updated successfully.")

# Step 5:perform PCA analysis with PLINK.
PLINK --bfile merged --pca 30 --out merged_pca

# run rye for genentic ancestry analysis
## Make pop2group.txt
Pop	Subgroup	Group
CEU	CEPH	European	
ESN	Nigerian	African
FIN	NorthernEuropean	European
GBR	WesternEuropean	European
GWD	Senegambian	African
IBS	Iberian	European
LWK	EastAfrican	African
MSL	Mende	African
PEL	SouthAmerica	American
TSI	Italian	European	
YRI	Nigerian	African

# Step 6:Run genetic_ancestry analysis using rye.R script_4
## make pop2group.txt which contain the  information of population ("https://github.com/healthdisparities/rye")
./rye.R --eigenvec=merged_pca.eigenvec --eigenval=merged_pca.eigenval --pop2group=pop2group.txt --rounds=5 --threads=2 --iter=5 --out=stroke